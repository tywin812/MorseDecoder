{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e97f46b",
   "metadata": {},
   "source": [
    "## Preparing & loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3b8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from torchaudio.functional import edit_distance\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8260e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/user/Desktop/Jupyter_and_projects/test_kontur/morse_decoder/mlflow_server/mlartifacts/models_info.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUDIO_FILES_DIR = '/mnt/c/Study/Python/Morse/morse_dataset'\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1af20",
   "metadata": {},
   "source": [
    "### Функции для обучения модели\n",
    "\n",
    "В этом разделе собраны основные функции, используемые в дальнейшем для обучения модели. <br>\n",
    "Если в каких-либо следующих разделах используется изменённый вариант функции, он представлен отдельно под другим именем прямо в соответствующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45674f86",
   "metadata": {},
   "source": [
    "Получение словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70fcfd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(words: pd.Series, blank: str = \"<blk>\") -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Строит словарь всех уникальных символов, встречающихся в целевом признаке.\n",
    "    Начальные индексы зарезервированы для служебных символов.\n",
    "    \"\"\"\n",
    "    vocab = {0: blank, 1: \"|\"}\n",
    "    all_chars = set(\"\".join(words.astype(str)))\n",
    "\n",
    "    for i, char in enumerate(sorted(all_chars), start=2):\n",
    "        vocab[i] = char\n",
    "\n",
    "    print(f\"Vocab is ready, size = {len(vocab)}\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3db01d",
   "metadata": {},
   "source": [
    "Предобработка аудио файлов с кэшированием полученных файлов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64ed00",
   "metadata": {},
   "source": [
    "Tokenizer для перевода символов в индексы и наоборот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45cc655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Класс для кодирования и декодирования строк на основе словаря символов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab: Dict[int, str]):\n",
    "        self.index_char = vocab\n",
    "        self.char_index = {char: index for index, char in self.index_char.items()}\n",
    "\n",
    "    def __call__(\n",
    "        self, chars: Union[str, List[str]]\n",
    "    ) -> Union[List[int], List[List[int]]]:\n",
    "        \"\"\"\n",
    "        Преобразует строку или список строк в список индексов.\n",
    "        \"\"\"\n",
    "        if isinstance(chars, str):\n",
    "            return [self.char_index.get(char, -1) for char in chars]\n",
    "        elif isinstance(chars, list):\n",
    "            return [self.__call__(t) for t in chars]\n",
    "        else:\n",
    "            raise ValueError(\"Expected list or str\")\n",
    "\n",
    "    def decode(\n",
    "        self, indexs: Union[List[int], List[List[int]]]\n",
    "    ) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Преобразует список индексов или список списков индексов обратно в строку(и).\n",
    "        \"\"\"\n",
    "        if isinstance(indexs, list):\n",
    "            if isinstance(indexs[0], list):\n",
    "                return [self.decode(i) for i in indexs]\n",
    "            else:\n",
    "                return \"\".join([self.index_char.get(idx, \"\") for idx in indexs])\n",
    "        else:\n",
    "            raise ValueError(\"Expected list or str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e7d55",
   "metadata": {},
   "source": [
    "Готовая функция для перевода выходов модели в конечный набор символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58956855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_to_tokens(\n",
    "    decoder: Callable,\n",
    "    model_output: torch.Tensor,\n",
    "    tokenizer: Callable,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Применяет декодер(beam_search) и tokenizer к выходу модели и возвращает расшифрованные строки.\n",
    "    \"\"\"\n",
    "\n",
    "    log_probs = nn.functional.log_softmax(model_output, dim=-1)\n",
    "    results = decoder(log_probs.contiguous())  # log_probs: [B, T, C]\n",
    "\n",
    "    decoded_sequences = []\n",
    "    for batch_result in results:\n",
    "        top_hypo = batch_result[0]\n",
    "        tokens = top_hypo.tokens.tolist()\n",
    "        decoded_sequence = tokenizer.decode(tokens)\n",
    "        decoded_sequence = decoded_sequence.strip(\"|\")\n",
    "        decoded_sequences.append(decoded_sequence)\n",
    "\n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e1568",
   "metadata": {},
   "source": [
    "Адаптация nn.CTCLoss под нашу задачу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a34136fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ctc(\n",
    "    model_output: torch.Tensor,  # Тензор [Batch, Time, Classes] - ожидается на device\n",
    "    targets: torch.Tensor,  # Тензор [sum(target_lengths)] - ожидается на CPU\n",
    "    target_lengths: torch.Tensor,  # Тензор [B] - ожидается на CPU\n",
    "    blank_id: int = 0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Вычисляет CTC-лосс (Connectionist Temporal Classification) между выходом модели и целевыми метками.\n",
    "\n",
    "    \"\"\"\n",
    "    log_output = F.log_softmax(model_output, dim=-1)\n",
    "    log_output = log_output.transpose(0, 1)\n",
    "\n",
    "    output_time_dim = log_output.shape[0]  # T'\n",
    "    batch_size = log_output.shape[1]  # B\n",
    "\n",
    "    output_lengths = torch.full(\n",
    "        size=(batch_size,), fill_value=output_time_dim, dtype=torch.long, device=\"cpu\"\n",
    "    )\n",
    "    targets_cpu = targets.cpu()\n",
    "    target_lengths_cpu = target_lengths.cpu()\n",
    "\n",
    "    loss = nn.CTCLoss(blank=blank_id, reduction=\"mean\", zero_infinity=True)\n",
    "\n",
    "    batch_loss = loss(\n",
    "        log_output.float(), targets_cpu, output_lengths, target_lengths_cpu\n",
    "    )\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34309d8a",
   "metadata": {},
   "source": [
    "Класс Dataset, принимающий имена файлов, transform для предобработки этих файлов и tokenizer для подготовки target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b7bc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для загрузки и обработки датасета\n",
    "\n",
    "    Атрибуты:\n",
    "        X_filenames (pd.Series): Список путей к аудио файлам.\n",
    "        y_texts (Optional[pd.Series]): Список целевых меток (строки).\n",
    "        transform (Optional[Callable]): Функция для преобразования аудио файлов в признаки.\n",
    "        tokenizer (Optional[Callable[[str], list]]): Tokenizer для преобразования текста в индексы.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_filenames: pd.Series,\n",
    "        y_texts: Optional[pd.Series] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        tokenizer: Optional[Callable[[str], list]] = None,\n",
    "    ):\n",
    "        self.X_filenames = X_filenames.reset_index(drop=True)\n",
    "        self.y_texts = y_texts.reset_index(drop=True) if y_texts is not None else None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X_filenames)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        filename = self.X_filenames[idx]\n",
    "        mel_features = self.transform(filename)  # [C, T] ([n_mels, time])\n",
    "\n",
    "        item = {\"input\": torch.tensor(mel_features, dtype=torch.float)}\n",
    "\n",
    "        if self.y_texts is not None:\n",
    "            text = self.y_texts[idx]\n",
    "            if self.tokenizer:\n",
    "                target = self.tokenizer(text)\n",
    "            else:\n",
    "                raise ValueError(\"Tokenizer needs for target encoding\")\n",
    "            item[\"target_text\"] = text\n",
    "            item[\"target\"] = torch.tensor(target, dtype=torch.long)\n",
    "            item[\"target_length\"] = len(target)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65d3bb",
   "metadata": {},
   "source": [
    "Функции для создания DataLoader с кастомным collate_fn для создания удобного batch при работе с CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73254815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_collate(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Функция для объединения элементов батча, с добавлением паддинга для последовательностей.\n",
    "    \"\"\"\n",
    "    inputs = [item[\"input\"].T for item in batch]  # [T, C]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True)  # [B, max_T, C]\n",
    "    padded_inputs = padded_inputs.transpose(1, 2)  # [B, C, T]\n",
    "\n",
    "    collated = {\n",
    "        \"input\": padded_inputs,\n",
    "    }\n",
    "\n",
    "    if \"target\" in batch[0]:\n",
    "        targets = torch.cat([item[\"target\"] for item in batch], dim=0)\n",
    "        target_lengths = torch.tensor(\n",
    "            [item[\"target_length\"] for item in batch], dtype=torch.long\n",
    "        )\n",
    "        collated[\"target_text\"] = [item[\"target_text\"] for item in batch]\n",
    "        collated[\"target\"] = targets\n",
    "        collated[\"target_length\"] = target_lengths\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "def data_loader(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    num_workers: int = 0,\n",
    "    drop_last: bool = True,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Функция для создания DataLoader с заданными параметрами.\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=dataloader_collate,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cda43",
   "metadata": {},
   "source": [
    "Основная функция для обучения модели и валидации результатов. <br>\n",
    "Для обучения используется CTC-loss. Валидация проводится с использованием beam-search декодирования, <br>\n",
    "после чего рассчитывается расстояние Левенштейна между предсказанным и истинным текстом.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4649bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    ctc_loss: Callable,\n",
    "    epochs: int,\n",
    "    metric: Callable,\n",
    "    decoder: Callable,\n",
    "    tokenizer: Callable,\n",
    "    val_loader: Union[DataLoader, None] = None,\n",
    "    scheduler: Union[Callable, None] = None,\n",
    ") -> Tuple[nn.Module, Dict[str, List]]:\n",
    "    \"\"\"\n",
    "    Обучение модели с CTC-loss, возможной валидацией и логированием метрик по эпохам.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device, non_blocking=True)\n",
    "    metrics = {\"train_loss\": [], \"train_metric\": [], \"val_metric\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n Epoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss = []\n",
    "        model.train()\n",
    "        with tqdm(\n",
    "            train_loader, desc=\"Training\", total=len(train_loader), dynamic_ncols=True\n",
    "        ) as pbar:\n",
    "            for batch in pbar:\n",
    "                inputs = batch[\"input\"].to(device, non_blocking=True)  # (B, C, T)\n",
    "                targets = batch[\"target\"]  # (sum_target_len,)\n",
    "                target_lengths = batch[\"target_length\"]  # (B,)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = model(inputs)\n",
    "                loss = ctc_loss(output, targets, target_lengths)\n",
    "\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                epoch_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        avg_epoch_loss = np.mean(epoch_loss)\n",
    "        metrics[\"train_loss\"].append(avg_epoch_loss)\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} with loss = {avg_epoch_loss:.4f}\"  # and metric_train={avg_epoch_train_metric:.4f}\"\n",
    "        )\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print(f\"Learning_rate = {scheduler.get_last_lr()}\")\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            epoch_val_metrics = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(\n",
    "                    val_loader, desc=f\"Validation Epoch {epoch + 1}\", leave=False\n",
    "                ):\n",
    "                    inputs = batch[\"input\"].to(device, non_blocking=True)  # (B, C, T)\n",
    "                    target_text = batch[\"target_text\"]\n",
    "                    # with autocast(device_type=device.type):\n",
    "                    predictions = model(inputs)\n",
    "\n",
    "                    emissions = predictions.detach().cpu().float()\n",
    "\n",
    "                    tokens_output = decoding_to_tokens(decoder, emissions, tokenizer)\n",
    "                    batch_val_metrics = [\n",
    "                        metric(pred, gt) for pred, gt in zip(tokens_output, target_text)\n",
    "                    ]\n",
    "                    epoch_val_metrics.append(np.mean(batch_val_metrics))\n",
    "\n",
    "            avg_epoch_val_metrics = np.mean(epoch_val_metrics)\n",
    "            metrics[\"val_metric\"].append(avg_epoch_val_metrics)\n",
    "            print(f\"Val Metric={avg_epoch_val_metrics:.4f}\")\n",
    "\n",
    "        else:\n",
    "            metrics[\"val_metric\"].append(None)\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481d236",
   "metadata": {},
   "source": [
    "Блоки для создания модели - сверточный блок и LSTM-блок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eff4c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Свёрточный блок с нормализацией и активацией ReLU.\n",
    "\n",
    "    Включает свёртку (Conv1d), слой нормализации (LayerNorm) и активацию ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            groups=groups,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x.transpose(1, 2)).transpose(1, 2)\n",
    "        return self.relu(x)\n",
    "\n",
    "\n",
    "class LstmBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Блок LSTM с нормализацией и dropout.\n",
    "\n",
    "    Включает двустороннюю LSTM (2 слоя), слой нормализации (LayerNorm) и слой dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.drop(self.norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb201a33",
   "metadata": {},
   "source": [
    "Класс модели - бэйзлана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9d02702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Базовая модель на основе свёрточный блок (ConvBlock),\n",
    "    LSTM блок (LstmBlock) и слой классификации (Linear).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: Tuple[int, int],\n",
    "        kernel_chanel: int,\n",
    "        conv_out_size: int,\n",
    "        vocab_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_part = nn.Sequential(\n",
    "            ConvBlock(\n",
    "                in_channels=input_size[0],\n",
    "                out_channels=kernel_chanel,\n",
    "                kernel_size=5,\n",
    "                padding=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.rnn_part = nn.Sequential(\n",
    "            LstmBlock(input_size=kernel_chanel, hidden_size=64, dropout=0.4)\n",
    "        )\n",
    "\n",
    "        self.clf = nn.Linear(\n",
    "            in_features=64 * 2,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_part(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.rnn_part(x)\n",
    "        return self.clf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd89cd",
   "metadata": {},
   "source": [
    "Инициализация начальных весов модели в зависимости от блока."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9357ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Инициализация весов для различных типов слоёв.\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Conv1d):\n",
    "        init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0)\n",
    "\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0)\n",
    "\n",
    "    elif isinstance(module, nn.LSTM):\n",
    "        for name, param in module.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                init.xavier_uniform_(param)\n",
    "            elif \"weight_hh\" in name:\n",
    "                init.orthogonal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                init.constant_(param, 0)\n",
    "\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        if module.elementwise_affine:\n",
    "            init.constant_(module.weight, 1.0)\n",
    "            init.constant_(module.bias, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752ec45",
   "metadata": {},
   "source": [
    "Отрисовка словаря с метриками, полученного после функции train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "930f70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    metrics_dict: dict,\n",
    "    title: str = \"Model Metrics\",\n",
    "    xlabel: str = \"Epochs\",\n",
    "    ylabel: str = \"Metric Value\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Функция для отображения метрик в виде графиков.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(list(metrics_dict.values())[0]) + 1)\n",
    "\n",
    "    for metric_name, metric_values in metrics_dict.items():\n",
    "        if metric_values:\n",
    "            plt.plot(epochs, metric_values, label=metric_name)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72aeb82",
   "metadata": {},
   "source": [
    "Функция для предсказания - получает список имен файлов, прогоняет через модель, через decoder и tokenizer и отдает полученные готовые предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da94958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(\n",
    "    model: nn.Module,\n",
    "    test_paths: pd.Series,\n",
    "    tokenizer: Tokenizer,\n",
    "    decoder: Callable,\n",
    "    batch_size: int = 32,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция для предсказания модели на тестовых данных.\n",
    "\n",
    "    Parameters:\n",
    "        model (nn.Module): Обученная модель для предсказаний.\n",
    "        test_paths (pd.Series): Имена тестовых аудиофайлов.\n",
    "        tokenizer (Tokenizer): Tokenizer для декодирования индексов.\n",
    "        decoder(Callable): Декодер для преобразования выходов модели в индексы словаря.\n",
    "        batch_size (int): Размер батча для DataLoader.\n",
    "        device (torch.device): Устройство (CPU или CUDA), на котором будет происходить вычисление.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame с декодированными предсказаниями.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Преобразование тестовых данных в датасет\n",
    "    test_dataset = MorseDataset(\n",
    "        X_filenames=test_paths, transform=transform, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Создание DataLoader\n",
    "    test_loader = data_loader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=\"Predicting\", total=len(test_loader)) as pbar:\n",
    "            for batch in pbar:\n",
    "                inputs = batch[\"input\"].to(device)  # [B, C, T]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                decoded_preds = decoding_to_tokens(decoder, outputs, tokenizer)\n",
    "\n",
    "                predictions.extend(decoded_preds)\n",
    "\n",
    "                pbar.set_postfix({\"Predictions\": len(predictions)})\n",
    "\n",
    "    return pd.DataFrame({\"id\": test_paths, \"message\": predictions})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16184e",
   "metadata": {},
   "source": [
    "Функция для сохранения метрик и моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c31edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_metrics(\n",
    "    model: nn.Module,\n",
    "    metrics: Dict[str, list],\n",
    "    model_path: Path,\n",
    "    metric_filename: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Сохраняет модель и метрики в указанные файлы.\n",
    "    \"\"\"\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "    clean_metrics = {key: [float(v) for v in values] for key, values in metrics.items()}\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/{metric_filename}\", \"w\") as fd:\n",
    "        json.dump(clean_metrics, fd)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75571ba8",
   "metadata": {},
   "source": [
    "Подготовим словарь с параметрами для обучения, кодирования и предобработки данных для последующего логирования на создания моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39811310",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"model\": {\n",
    "        \"type\": \"ModelBase\",\n",
    "        \"input_size\": [80, 251],\n",
    "        \"kernel_chanel\": 128,\n",
    "        \"conv_out_size\": 128,\n",
    "        \"vocab_size\": 46,\n",
    "        \"init_weights\": \"init_weights()\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"lr\": 0.01,\n",
    "    },\n",
    "    \"scheduler\": {\"step_size\": \"-\", \"gamma\": \"-\"},\n",
    "    \"transform\": {\n",
    "        \"type\": \"path_to_melspect_cached()\",\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"type\": \"ctc_decoder\",\n",
    "        \"lexicon\": None,\n",
    "        \"tokens_count\": 46,\n",
    "        \"beam_size\": 3,\n",
    "        \"nbest\": 1,\n",
    "        \"blank_token\": \"<blk>\",\n",
    "    },\n",
    "    \"dataloader\": {\"batch_size\": 64, \"num_workers\": 0},\n",
    "    \"training\": {\"epochs\": 50},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f930d",
   "metadata": {},
   "source": [
    "\n",
    "## Boost, prune & more cnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cac61",
   "metadata": {},
   "source": [
    "**В данном эксперименте пробуем сделать предобработку более тщательной: используем мел-спектрограммы с окном 512, шагом 128 и 64 мел-бинами, <br> \n",
    "после чего возводим спектрограмму в степень, нормализуем и оставляем диапазон в 20 мел-бинов вокруг самого мощного сигнала. <br> \n",
    "Архитектуру модели сохраняем с тремя сверточными блоками, но немного уменьшаем их параметры — входные данные теперь меньше по размеру.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c23d3d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79356fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_blackmel_cached(\n",
    "    file_name: str,\n",
    "    audio_files_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    overwrite: bool = False,\n",
    ") -> np.ndarray:\n",
    "    cache_path = cache_dir / (file_name.replace(\".opus\", \".npy\"))\n",
    "\n",
    "    if cache_path.exists() and not overwrite:\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    waveform, sr = torchaudio.load(audio_files_dir / file_name)\n",
    "\n",
    "    mel = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sr, n_fft=512, hop_length=128, n_mels=64\n",
    "    )(waveform)\n",
    "    boost_mel = mel**5\n",
    "    boost_mel = boost_mel / boost_mel.max()\n",
    "    mel_db = torchaudio.transforms.AmplitudeToDB(top_db=64)(boost_mel)\n",
    "    mel_db = mel_db.squeeze(0).cpu().numpy()\n",
    "\n",
    "    peak = np.argmax(mel_db.mean(axis=1))\n",
    "    start = max(peak - 10, 0)\n",
    "    end = min(peak + 10, mel_db.shape[0])\n",
    "    fresh_mel = mel_db[start:end]\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    np.save(cache_path, fresh_mel)\n",
    "    return fresh_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN_small(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: Tuple[int, int],\n",
    "        kernel_chanel: int,\n",
    "        conv_out_size: int,\n",
    "        vocab_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_part = nn.Sequential(\n",
    "            ConvBlock(\n",
    "                in_channels=input_size[0],\n",
    "                out_channels=kernel_chanel,\n",
    "                kernel_size=5,\n",
    "                padding=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "            ConvBlock(\n",
    "                in_channels=kernel_chanel,\n",
    "                out_channels=conv_out_size,\n",
    "                kernel_size=4,\n",
    "                padding=0,\n",
    "                stride=2,\n",
    "            ),\n",
    "            ConvBlock(\n",
    "                in_channels=conv_out_size,\n",
    "                out_channels=conv_out_size,\n",
    "                kernel_size=3,\n",
    "                padding=0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.rnn_part = nn.Sequential(\n",
    "            LstmBlock(input_size=conv_out_size, hidden_size=64, dropout=0.4),\n",
    "        )\n",
    "\n",
    "        self.clf = nn.Linear(\n",
    "            in_features=64 * 2,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_part(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.rnn_part(x)\n",
    "        return self.clf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2130886",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=list(full_vocab.values()),\n",
    "    beam_size=3,\n",
    "    nbest=1,\n",
    "    blank_token=\"<blk>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ad14996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<blk>', 1: '|', 2: ' ', 3: '#', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: 'А', 15: 'Б', 16: 'В', 17: 'Г', 18: 'Д', 19: 'Е', 20: 'Ж', 21: 'З', 22: 'И', 23: 'Й', 24: 'К', 25: 'Л', 26: 'М', 27: 'Н', 28: 'О', 29: 'П', 30: 'Р', 31: 'С', 32: 'Т', 33: 'У', 34: 'Ф', 35: 'Х', 36: 'Ц', 37: 'Ч', 38: 'Ш', 39: 'Щ', 40: 'Ъ', 41: 'Ы', 42: 'Ь', 43: 'Э', 44: 'Ю', 45: 'Я'}\n",
      "{'<blk>': 0, '|': 1, ' ': 2, '#': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, 'А': 14, 'Б': 15, 'В': 16, 'Г': 17, 'Д': 18, 'Е': 19, 'Ж': 20, 'З': 21, 'И': 22, 'Й': 23, 'К': 24, 'Л': 25, 'М': 26, 'Н': 27, 'О': 28, 'П': 29, 'Р': 30, 'С': 31, 'Т': 32, 'У': 33, 'Ф': 34, 'Х': 35, 'Ц': 36, 'Ч': 37, 'Ш': 38, 'Щ': 39, 'Ъ': 40, 'Ы': 41, 'Ь': 42, 'Э': 43, 'Ю': 44, 'Я': 45}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab=full_vocab)\n",
    "\n",
    "print(tokenizer.index_char)\n",
    "print(tokenizer.char_index)\n",
    "\n",
    "transform = partial(\n",
    "    path_to_blackmel_cached,\n",
    "    audio_files_dir=AUDIO_FILES_DIR,\n",
    "    cache_dir=Path(\"./blackmels_cnn_cache\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430cddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelCNN(\n",
       "  (conv_part): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (conv): Conv1d(20, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (conv): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (rnn_part): Sequential(\n",
       "    (0): LstmBlock(\n",
       "      (lstm): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (clf): Linear(in_features=128, out_features=46, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_blackmel_model = ModelCNN_small(\n",
    "    input_size=[20, 501],\n",
    "    kernel_chanel=32,\n",
    "    conv_out_size=64,\n",
    "    vocab_size=len(full_vocab),\n",
    ")\n",
    "torch.manual_seed(SEED)\n",
    "cnn_blackmel_model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn_blackmel_model.parameters(), lr=0.005)\n",
    "\n",
    "step_lr = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "cnn_blackmel_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "cnn_blackmel_model, cnn_blackmel_metrics = train_model(\n",
    "    model=cnn_blackmel_model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    ctc_loss=loss_ctc,\n",
    "    epochs=35,\n",
    "    metric=edit_distance,\n",
    "    decoder=beam_search_decoder,\n",
    "    tokenizer=tokenizer,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=step_lr,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
